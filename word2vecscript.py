# -*- coding: utf-8 -*-
"""WORD2VECSCRIPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sxvup7SqlbwBVXcpQaRJz5MWZfB306wo
"""

from IPython.display import clear_output
!pip install --upgrade tensorflow-gpu
clear_output()

#Downloads and extract Dataset to local, wait for download, i dont want to put a progress bar here sorry
#You can run this on google colab for get faster downloads speeds
import os
import zipfile
import requests

if(not os.path.exists("./Datasets/MoviLens.zip")):

  resp = requests.get("http://files.grouplens.org/datasets/movielens/ml-latest-small.zip")

  os.mkdir("./Datasets")

  with open("./Datasets/MoviLens.zip", "wb") as f:
    f.write(resp.content)

  with zipfile.ZipFile("./Datasets/MoviLens.zip", "r") as zip_ref:
    zip_ref.extractall("./Datasets")

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os

clear_output()

from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

#for text pre-processing
import re, string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

#for model-building
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score

# bag of words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

#for word embedding
import gensim
from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets
import requests       #to send the request to the URL
from bs4 import BeautifulSoup #to get the content in the form of HTML
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import numpy as np  # to count the values (in our case)

import nltk
nltk.download('stopwords')
nltk.download('corpora')
nltk.download('omw-1.4')

#1. Common text preprocessing
text = "   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  "

#convert to lowercase and remove punctuations and characters and then strip
def preprocess(text):
    text = text.lower() #lowercase text
    text=text.strip()  #get rid of leading/trailing whitespace
    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful
    text = re.sub('\s+', ' ', text)  #Remove extra space and tabs
    text = re.sub(r'\[[0-9]*\]',' ',text) #[0-9] matches any digit (0 to 10000...)
    text=re.sub(r'[^\w\s]', '', str(text).lower().strip())
    text = re.sub(r'\d',' ',text) #matches any digit from 0 to 100000..., \D matches non-digits
    text = re.sub(r'\s+',' ',text) #\s matches any whitespace, \s+ matches multiple whitespace, \S matches non-whitespace

    return text

text=preprocess(text)
print(text)  #text is a string

def preprocess(text):
    text = text.lower() #lowercase text
    text=text.strip()  #get rid of leading/trailing whitespace
    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful
    text = re.sub('\s+', ' ', text)  #Remove extra space and tabs
    text = re.sub(r'\[[0-9]*\]',' ',text) #[0-9] matches any digit (0 to 10000...)
    text=re.sub(r'[^\w\s]', '', str(text).lower().strip())
    text = re.sub(r'\d',' ',text) #matches any digit from 0 to 100000..., \D matches non-digits
    text = re.sub(r'\s+',' ',text) #\s matches any whitespace, \s+ matches multiple whitespace, \S matches non-whitespace

    return text

def stopword(string):
    a= [i for i in string.split() if i not in stopwords.words('english')]
    return ' '.join(a)

snow = SnowballStemmer('english')
def stemming(string):
    a=[snow.stem(i) for i in word_tokenize(string) ]
    return " ".join(a)
#text=stemming(text)
#print(text)

wl = WordNetLemmatizer()

def get_wordnet_pos(tag):
  if tag.startswith('J'):
      return wordnet.ADJ
  elif tag.startswith('V'):
      return wordnet.VERB
  elif tag.startswith('N'):
      return wordnet.NOUN
  elif tag.startswith('R'):
      return wordnet.ADV
  else:
      return wordnet.NOUN

def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return " ".join(a)

def finalpreprocess(string):
    return lemmatizer(stopword(preprocess(string)))

#Loads Dataset, we only need ratings.csv and movies.csv files, we can drop timestamp and genres for now
ratings_df = pd.read_csv("./Datasets/ml-latest-small/ratings.csv").drop(["timestamp"], axis=1)
movies_df = pd.read_csv("TELIKOTITLES.csv").drop(["genres"], axis=1)

ml_df = ratings_df.merge(movies_df, on="movieId")
ml_df = ml_df.reindex(columns=["userId", "movieId", "title", "rating"])
#ml_df.head()
ml_df.shape
#movies_df.shape

movies_dff = pd.read_csv('TELIKOSCRIPTS.csv')
movies_dff.drop_duplicates(subset ="title",
                     keep = False, inplace = True)

#Encode ratings, now will encode the probability of relevance of the item for the user
#The negative sampling ratio under this setup is almost 2.5, but can be better have a setup of 5.0 ratio
ml_df["relevance"] = (ml_df["rating"] >= 5.0).astype(dtype=float)

#Set Ids as categorical data
ml_df["userId"] = ml_df["userId"].astype("category").cat.codes.values
ml_df["movieId"] = ml_df["movieId"].astype("category").cat.codes.values

ml_df.head()

full_df = ml_df

fullmat = full_df.pivot_table(index='userId',columns='title',values='relevance').fillna(0)

fullmat.shape

fullMlist=[]

#take the names of the movies from fullmat
for col in fullmat.columns:
  fullMlist.append(col)

len(fullMlist)

print(fullMlist)

text_dff = pd.DataFrame(movies_dff)

full_list = fullmat.values.tolist()

list_full=[]
a=[]

#convert values of list_full from double to integer
for i in full_list:
  a.clear()
  for j in i:
    a.append(int(j))
  list_full.append(a)

print(list_full[67])

movies_dff['clean_text'] = movies_dff['Script'].apply(lambda x: finalpreprocess(x))

#movies_dff
movies_dff['clean_text_tok']=[nltk.word_tokenize(i) for i in movies_dff['clean_text']] #convert preprocessed sentence to tokenized sentence
model = Word2Vec(movies_dff['clean_text_tok'],min_count=1)

w2v = dict(zip(model.wv.index2word, model.wv.syn0))  #combination of word and its vector

#for converting sentence to vectors/numbers from word vectors result by Word2Vec
class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

total_accuracy = []

for i in list_full:
  movies_dff['target'] = i
  testPercentage = 0.2
  df_train, df_test = train_test_split(movies_dff, test_size=testPercentage, random_state=42, shuffle=True)
  X_train, X_val, y_train, y_val = train_test_split(df_train["clean_text"],
                                                  df_train["target"],
                                                  test_size=0.2,
                                                  shuffle=True)
  X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec
  X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec

  #TF-IDF
  # Convert x_train to vector since model can only run on numbers and not words- Fit and transform
  tfidf_vectorizer = TfidfVectorizer(use_idf=True)
  X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec
  # Only transform x_test (not fit and transform)
  X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will
  #change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-
  #fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without
  #it, and the have compatible


  #Word2vec
  # Fit and transform
  modelw = MeanEmbeddingVectorizer(w2v)
  X_train_vectors_w2v = modelw.transform(X_train_tok)
  X_val_vectors_w2v = modelw.transform(X_val_tok)

  #-------------------------------------------------------------------------------------
  #FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)

  lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')
  lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model

  #Predict y value for test dataset
  y_predict = lr_tfidf.predict(X_val_vectors_tfidf)
  y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]

  total_accuracy.append(accuracy_score(y_val,y_predict))
  #-------------------------------------------------------------------------------------

  movies_dff.drop(['target'], axis = 1)

print(total_accuracy)

def Average(lst):
    return sum(lst) / len(lst)

print(Average(total_accuracy))